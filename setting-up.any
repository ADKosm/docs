#language anatomy

\use{\load{concourse/docs}}

\template{\load{concourse/docs-template}}

\title{Setting Up}{setting-up}

So you want to get yourself a Concourse. There are three supported ways to get
going: a local VirtualBox VM via \hyperlink{https://vagrantup.com}{Vagrant},
standalone binaries that you can run (almost) anywhere, and
\hyperlink{https://bosh.io}{BOSH} for all your clustering needs.

All distributions of Concourse are equivalent in feature set, the only
difference being operational concerns like scaling up your workers.

\table-of-contents

\section{Local VM with Vagrant}{vagrant}{
  \omit-children-from-table-of-contents

  The quickest way to spin up a fully functioning Concourse is with
  \hyperlink{https://vagrantup.com}{Vagrant}.

  To get started, run the following in any directory:

  \codeblock{bash}{
  vagrant init concourse/lite # creates ./Vagrantfile
  vagrant up                  # downloads the box and spins up the VM
  }

  The web server will be running at
  \hyperlink{http://192.168.100.4:8080}{192.168.100.4:8080}.

  While this isn't exactly production ready, it may be enough depending on your
  project's needs. Given that Concourse is stateless, you can always hoist your
  pipeline onto a bigger installation when you're ready, so there's little risk
  in sticking with the Vagrant boxes while you figure things out.
}

\section{Standalone Binaries}{binaries}{
  \omit-children-from-table-of-contents

  At some point you may want to start putting Concourse on to real hardware. A
  binary distribution is available in the \reference{downloads}{downloads}
  section.

  These binaries are fairly self-contained, making them ideal for tossing onto
  a VM by hand or orchestrating them with Docker, Chef, or other ops tooling.

  \section{Prerequisites}{
    \list{
      Grab the appropriate binary for your platform from the
      \reference{downloads}{downloads} section.
    }{
      For the Linux distribution you'll need kernel version 3.19 or above, plus
      \code{iptables} and \code{ip} commands. Windows and Darwin don't really
      need anything special.
    }{
      PostgresSQL 9.3+
    }
  }

  \section{Generating Keys}{generating-keys}{
    To run Concourse securely you'll need to generate 3 private keys (well, 2,
    plus 1 for each worker):

    \definitions{
      \item{\code{session_signing_key} (currently must be RSA)}{
        Used for signing user session tokens, and by the TSA to sign its own
        tokens in the requests it makes to the ATC.
      }

      \item{\code{host_key}}{
        Used for the TSA's SSH server. This is the key whose fingerprint you
        see when the \code{ssh} command warns you when connecting to a host it
        hasn't seen before.
      }

      \item{\code{worker_key} (one per worker)}{
        Used for authorizing worker registration. There can actually be an
        arbitrary number of these keys; they are just listed to authorize
        worker SSH access.
      }
    }

    To generate these keys, run:

    \codeblock{sh}{
    ssh-keygen -t rsa -f host_key -N ''
    ssh-keygen -t rsa -f worker_key -N ''
    ssh-keygen -t rsa -f session_signing_key -N ''
    }

    ...and we'll also start on an \code{authorized_keys} file, currently
    listing this initial worker key:

    \codeblock{sh}{
    cp worker_key.pub authorized_worker_keys
    }
  }

  \section{Starting the Web UI & Scheduler}{
    The \code{concourse} binary embeds the
    \hyperlink{https://github.com/concourse/atc}{ATC} and
    \hyperlink{https://github.com/concourse/tsa}{TSA} components, available as
    the \code{web} subcommand.

    The ATC is the component responsible for scheduling builds, and also serves
    as the web UI and API.

    The TSA provides a SSH interface for securely registering workers, even if
    they live in their own private network.

    \section{Single node, local Postgres}{
      The following command will spin up the ATC, listening on port
      \code{8080}, with some basic auth configured, and a TSA listening on port
      \code{2222}.

      \codeblock{sh}{
      concourse web \\
        --basic-auth-username myuser \\
        --basic-auth-password mypass \\
        --session-signing-key session_signing_key \\
        --tsa-host-key host_key \\
        --tsa-authorized-keys authorized_worker_keys
      }

      This assumes you have a local Postgres server running on the default port
      (\code{5432}) with an \code{atc} database, accessible by the current
      user. If your database lives elsewhere, just specify the
      \code{--postgres-data-source} flag, which is also demonstrated below.
    }

    \section{Cluster with remote Postgres}{
      The ATC can be scaled up for high availability, and they'll also roughly
      share their scheduling workloads, using the database to synchronize.

      The TSA can also be scaled up, and requires no database as there's no
      state to synchronize (it just talks to the ATC).

      A typical configuration with multiple ATC+TSA nodes would have them
      sitting behind a load balancer, forwarding port \code{80} to \code{8080}
      and \code{2222} to \code{2222}. You may also want to configure SSL if
      your load balancer supports it.

      To run multiple \code{web} nodes, you'll need to pass the following
      flags:

      \list{
        \code{--postgres-data-source} should all refer to the same database
      }{
        \code{--peer-url} should be a URL used to reach the individual ATC,
        from other ATCs, i.e. a URL usable within their private network
      }{
        \code{--external-url} should be the URL used to reach \italic{any} ATC,
        i.e. the URL to your load balancer
      }

      For example:

      Node 0:

      \codeblock{sh}{
      concourse web \\
        --basic-auth-username myuser \\
        --basic-auth-password mypass \\
        --session-signing-key session_signing_key \\
        --tsa-host-key host_key \\
        --tsa-authorized-keys authorized_worker_keys \\
        --postgres-data-source postgres://user:pass@10.0.32.0/concourse \\
        --external-url https://ci.example.com \\
        --peer-url http://10.0.16.10:8080
      }

      Node 1 (only difference is \code{--peer-url}):

      \codeblock{sh}{
      concourse web \\
        --basic-auth-username myuser \\
        --basic-auth-password mypass \\
        --session-signing-key session_signing_key \\
        --tsa-host-key host_key \\
        --tsa-authorized-keys authorized_worker_keys \\
        --postgres-data-source postgres://user:pass@10.0.32.0/concourse \\
        --external-url https://ci.example.com \\
        --peer-url http://10.0.16.11:8080
      }
    }
  }

  \section{Running a worker}{
    To spin up a
    \hyperlink{https://github.com/cloudfoundry-incubator/garden}{Garden} server
    and register it with your Concourse cluster at \code{ci.example.com}, run:

    \codeblock{sh}{
    sudo concourse worker \\
      --work-dir /opt/concourse/worker \\
      --peer-ip 10.0.48.0 \\
      --tsa-host ci.example.com \\
      --tsa-public-key host_key.pub \\
      --tsa-worker-private-key worker_key
    }

    Note that the worker must be run as \code{root}, as it orchestrates
    containers.

    The \code{--work-dir} flag specifies where container data should be placed;
    make sure it has plenty of disk space available, as it's where all the disk
    usage across your builds and resources will end up.

    The \code{--peer-ip} flag specifies the IP of this worker reachable by
    other \code{web} nodes in your cluster. If your worker is in a private
    network, this flag can be omitted, and the TSA will forward connections to
    the worker via a SSH tunnel instead.

    The \code{--tsa-host} refers to wherever your TSA node is listening, by
    default on port \code{2222} (pass \code{--tsa-port} if you've configured it
    differently). This may be an address to a load balancer if you're running
    multiple \code{web} nodes, or just an IP, perhaps \code{127.0.0.1} if
    you're tinkering.

    The \code{--tsa-public-key} flag is used to ensure we're connecting to the
    TSA we should be connecting to, and is used like \code{known_hosts} with
    the \code{ssh} command. Refer to \reference{generating-keys} if you're not
    sure what this means.

    The \code{--tsa-worker-private-key} flag specifies the key to use when
    authenticating to the TSA. Refer to \reference{generating-keys} if you're
    not sure what this means.

    If you are starting a windows worker you will likely need \code{--bind-ip
    127.0.0.1} as well.
  }
}

\section{Clusters with BOSH}{bosh}{
  \omit-children-from-table-of-contents

  Deploying Concourse with \hyperlink{https://bosh.io}{BOSH} provides a
  scalable cluster with health management and rolling upgrades.

  While it's a bit more of an investment to learn another tool, you'll find a
  lot of similarities between BOSH and Concourse. If you find yourself wanting
  to deploy arbitrary software across arbitrary infrastructures and manage them
  at scale, learning BOSH may prove very useful.

  \section{Prepping the Environment}{prepping-bosh}{
    To go from nothing to a BOSH managed Concourse, you'll need to do the following:

    \list{
      \hyperlink{http://bosh.io/docs/init.html}{Initialize a BOSH director},
      which is the server that orchestrates BOSH deployments.
    }{
      \hyperlink{http://bosh.io/docs/cloud-config.html}{Set up your Cloud
      Config}, which describes the infrastructure to which Concourse will be
      deployed.
    }{
      \hyperlink{http://bosh.io/docs/uploading-stemcells.html}{Upload the
      stemcell}, which contains the base image used for VMs managed by BOSH.
    }

    You can skip some of this if you already have a working BOSH director
    running BOSH v255.4 and up.
  }

  \section{Deploying Concourse}{
    Once you've got all that set up, download the releases listed for your
    version of Concourse from the \reference{Downloads} section, and
    \hyperlink{http://bosh.io/docs/uploading-releases.html}{upload them to the
    BOSH director}.

    Next you'll need a Concourse
    \hyperlink{http://bosh.io/docs/deployment.html}{BOSH deployment manifest}. An
    example manifest is below; you'll want to replace the \code{REPLACE_ME} bits
    with whatever values are appropriate.

    Note that the VM types and network names used in this manifest must be
    present in your Cloud Config. Consult \reference{prepping-bosh} if you
    haven't set it up yet.

    \codeblock{yaml}{
      ---
      name: concourse

      # replace with `bosh status --uuid`
      director_uuid: REPLACE_ME

      releases:
      - name: concourse
        version: latest
      - name: garden-linux
        version: latest

      stemcells:
      - alias: trusty
        os: ubuntu-trusty
        version: latest

      instance_groups:
      - name: web
        instances: 1
        vm_type: web
        stemcell: trusty
        azs: [z1]
        networks: [\{name: private\}]
        jobs:
        - name: atc
          release: concourse
          properties:
            # replace with your CI's externally reachable URL, e.g. https://ci.foo.com
            external_url: REPLACE_ME

            # replace with username/password, or configure GitHub auth
            basic_auth_username: REPLACE_ME
            basic_auth_password: REPLACE_ME

            postgresql_database: &atc_db atc
        - name: tsa
          release: concourse

      - name: db
        instances: 1
        vm_type: database
        stemcell: trusty
        persistent_disk_type: database
        azs: [z1]
        networks: [\{name: private\}]
        jobs:
        - name: postgresql
          release: concourse
          properties:
            databases:
            - name: *atc_db
              # make up a role and password
              role: REPLACE_ME
              password: REPLACE_ME

      - name: worker
        instances: 1
        vm_type: worker
        stemcell: trusty
        azs: [z1]
        networks: [\{name: private\}]
        jobs:
        - name: groundcrew
          release: concourse
        - name: baggageclaim
          release: concourse
        - name: garden
          release: garden-linux
          properties:
            garden:
              listen_network: tcp
              listen_address: 0.0.0.0:7777

      update:
        canaries: 1
        max_in_flight: 1
        serial: false
        canary_watch_time: 1000-60000
        update_watch_time: 1000-60000
    }

    You may also want to consult the property descriptions for your version of
    Concourse at
    \hyperlink{http://bosh.io/releases/github.com/concourse/concourse}{bosh.io}
    to see what properties you can/should tweak.

    Once you've got a manifest, just
    \hyperlink{http://bosh.io/docs/deploying.html}{deploy it}!
  }

  \section{Reaching the web UI}{
    This really depends on your infrastructure. If you're deploying to AWS you
    may want to configure the \code{web} VM type to register with an ELB,
    mapping port \code{80} to \code{8080} and \code{2222} to \code{2222}. You
    may also want to configure SSL termination, which is currently not handled
    by Concourse.

    Otherwise you may want to configure \code{static_ips} for the \code{web}
    instance group and just reach the web UI directly.
  }

  \section{Upgrading & Maintaining Concourse}{
    With BOSH, the deployment manifest is the source of truth. This is very
    similar to Concourse's own philosophy, where all pipeline configuration is
    defined in a single declarative document.

    So, to add more workers or web nodes, just change the \code{instances}
    value for the instance group and re-deploy.

    To upgrade, just upload the new releases and re-deploy.
  }
}
