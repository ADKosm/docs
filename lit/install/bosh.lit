\title{Clusters with BOSH}{clusters-with-bosh}

\use-plugin{concourse-docs}

\omit-children-from-table-of-contents

Deploying Concourse with \link{BOSH}{https://bosh.io} provides a scalable
cluster with health management and rolling upgrades.

If you're not yet familiar with BOSH, learning it will be a bit of an
investment, but it should pay off in spades. There are a lot of parallels
between the philosophy of BOSH and Concourse.

\section{
  \title{Learning BOSH}

  If you've never used BOSH before, you may want to first go through its
  introductory material:

  \list{
    \link{What is BOSH?}{http://bosh.io/docs}
  }{
    \link{General architecture}{http://bosh.io/docs/bosh-components.html}
  }{
    \link{Terminology}{http://bosh.io/docs/terminology.html}
  }

  To tie the terminology together: Concourse is packaged as a \italic{release},
  whose source repository lives at
  \link{concourse/concourse}{https://github.com/concourse/concourse} on GitHub.

  Releases and \italic{stemcells} (operating system images) are put together to
  form a \italic{deployment manifest} (a YAML file, similar to a Concourse
  pipeline, which describes what jobs to run where, and with what properties).

  A BOSH director is given a \italic{cloud config}, which is another YAML file
  describing IaaS-specific things like network names and VM types. This file is
  changed a lot less often than your deployment manifests.

  The BOSH director is then used to deploy the deployment manifest, which is
  what brings the "abstract" (releases, deployment manifests, a bunch of YAML)
  to the "concrete" (software running on VMs running on an IaaS of your
  choice). The director is then used to maintain the deployment, allowing you
  to e.g. SSH into VMs, re-create them, perform a rolling upgrade, manage
  persistent state, recover from IaaS instability, etc. etc.
}

\section{
  \title{Deploying Concourse with BOSH}

  The quickest and easiest way to get going is to use our
  \link{\code{concourse-deployment} "cluster"
  recipes}{https://github.com/concourse/concourse-deployment/tree/master/cluster}.
  This repository contains a base manifest, some example cloud configs, and
  useful \link{Operations files}{http://bosh.io/docs/cli-ops-files.html} for
  deploying and maintaining the latest version of Concourse with BOSH. In the
  future, it will also include useful deployments to provide things like
  credential management and metrics.

  The \code{concourse-deployment} repo covers the following steps:

  \list{
    Setting up your cloud config: there's a \code{cloud_configs} directory
    containing various examples. Some of them may work verbatim. If none of
    them apply, though, you're on your own; you should consult
    \link{bosh.io}{http://bosh.io/docs/cloud-config.html} for IaaS-specific
    information.
  }{
    Creating the manifest: that's kind of the point of the repo. Any required
    property changes will be made to the repository as soon as they're needed,
    so when a new Concourse comes out you should be able to just
    re-\code{deploy}.
  }{
    Uploading the releases: the manifest will include them by URL, version, and
    sha1, allowing the director to download them for you.
  }{
    Configuring \reference{authentication} and other fancier setups should be
    available as Operations files under the
    \link{\code{operations/}}{https://github.com/concourse/concourse-deployment/tree/master/cluster/operations}
    tree.
  }

  However, it does \italic{not} cover the following:

  \list{
    Initializing your BOSH director. To go from nothing to a BOSH managed
    Concourse, you'll first need to \link{initialize a BOSH
    director}{http://bosh.io/docs/init.html}.
  }{
    Uploading the stemcells. Head over to
    \link{bosh.io/stemcells}{http://bosh.io/stemcells} and grab the appropriate
    one for your IaaS, and \link{upload it to your
    director}{http://bosh.io/docs/uploading-stemcells.html}.
  }

  These steps are general to anyone using BOSH, and so are not replicated in
  our documentation.

  Another handy reference is the \link{release documentation on
  bosh.io}{https://bosh.io/releases/github.com/concourse/concourse}, which
  makesit easy to peruse the available properties for each job. We
  intentionally keep all documentation and examples within the release itself
  to ensure there is no out-of-date documentation.

  You may also want to consult \reference{authentication} for configuring
  something other than basic auth for the \reference{main-team}{\code{main}
  team}.
}

\section{
  \title{Reaching the web UI}

  This really depends on your infrastructure. If you're deploying to AWS you
  may want to configure the \code{web} VM type to register with an ELB,
  mapping port \code{80} to \code{8080}, \code{443} to \code{4443} (if you've
  configured TLS), and \code{2222} to \code{2222}.

  Otherwise you may want to configure \code{static_ips} for the \code{web}
  instance group and just reach the web UI directly.
}

\section{
  \title{Upgrading & maintaining Concourse}

  With BOSH, the deployment manifest is the source of truth. This is very
  similar to Concourse's own philosophy, where all pipeline configuration is
  defined in a single declarative document.

  So, to add more workers or web nodes, just change the \code{instances}
  value for the instance group and re-run \code{bosh deploy}.

  To upgrade, just upload the new releases and re-run \code{bosh deploy}.
}

\section{
  \title{Supporting external workers}{configuring-bosh-tsa}

  If you need workers that run outside of your BOSH managed deployment (e.g.
  for testing with iOS or in some special network), you'll need to make some
  tweaks to the default configuration of the \code{tsa} job.

  The \reference{component-tsa} is the entryway for workers to join the
  cluster. For every new worker key pair, the TSA will be told to authorize
  its public key, and the workers must also know the TSA's public key ahead
  of time, so they know who they're connecting to.

  \section{
    \title{Authorizing worker keys}

    Workers are authorized into the cluster by listing their public key in the
    \code{authorized_keys} property on the \code{tsa} job, and configuring the
    worker's \code{tsa.worker_key} property on the \code{groundcrew} job to
    register with the corresponding private key.

    To do so, set the following properties:

    \definitions{
      \definition{\code{authorized_keys} on the \code{tsa} job}{
        the array of public keys to authorize
      }
    }{
      \definition{\code{tsa.worker_key} on the \code{groundcrew} job}{
        the private key for the worker to use when accessing the TSA
      }
    }{
      \definition{\code{tsa.host} and \code{tsa.host_public_key} on the \code{groundcrew} job}{
        if the worker is in a separate deployment, these must be configured
        to reach the TSA
      }
    }{
      \definition{\code{garden.forward_address} on the \code{groundcrew} job}{
        if the worker is in a separate deployment, this must be the
        locally-reachable Garden address to forward through the TSA; e.g.:
        127.0.0.1:7777
      }
    }{
      \definition{\code{baggageclaim.forward_address} on the \code{groundcrew} job}{
        if the worker is in a separate deployment, this must be the
        locally-reachable Baggageclaim address to forward through the TSA;
        e.g.: 127.0.0.1:7788
      }
    }

    After setting these properties run \code{bosh deploy} to make the changes
    take place.
  }

  \section{
    \title{Making the TSA reachable}

    Typically the TSA and ATC will both be colocated in the same instance
    group. This way a single load balancer can be used with the following
    scheme:

    \list{
      expose port \code{443} to \code{8080} (ATC's HTTP port) via SSL
    }{
      expose port \code{2222} to \code{2222} (TSA's SSH port) via TCP
    }

    Be sure to update any relevant security group rules (or equivalent in
    non-AWS environments) to permit both access from the outside world to
    port \code{2222} on your load balancer, \italic{and} access from the load
    balancer to port \code{2222} on your TSA + ATC instances.

    The BOSH deployment manifest would then colocate both jobs together, like
    so:

    \codeblock{yaml}{{
    instance_groups:
    - name: web
      vm_type: web_lb
      jobs:
      - name: atc
        release: concourse
        # ...
      - name: tsa
        release: concourse
        # ...
    }}

    In AWS, the \code{web_lb} VM type would then configure
    \code{cloud_properties.elbs} to auto-register instances of \code{web}
    with an ELB. See \link{the AWS CPI
    docs}{https://bosh.io/docs/aws-cpi.html#resource-pools} for more
    information.
  }
}
