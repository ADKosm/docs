\title{Clusters with BOSH}{clusters-with-bosh}

\use-plugin{concourse-docs}

\omit-children-from-table-of-contents

Deploying Concourse with \link{BOSH}{https://bosh.io} provides a scalable
cluster with health management and rolling upgrades.

If you're not yet familiar with BOSH, learning it will be a bit of an
investment, but it should pay off in spades. There are a lot of parallels
between the philosophy of BOSH and Concourse.

\section{
  \title{Prepping the Environment}{prepping-bosh}

  To go from nothing to a BOSH managed Concourse, you'll need to do the following:

  \list{
    \link{Initialize a BOSH director}{http://bosh.io/docs/init.html}, which
    is the server that orchestrates BOSH deployments.
  }{
    \link{Set up your Cloud Config}{http://bosh.io/docs/cloud-config.html},
    which describes the infrastructure to which Concourse will be deployed.
  }{
    \link{Upload the stemcell}{http://bosh.io/docs/uploading-stemcells.html},
    which contains the base image used for VMs managed by BOSH.
  }

  You can skip some of this if you already have a working BOSH director
  running BOSH v255.4 and up.
}

\section{
  \title{Deploying Concourse}

  Once you've got all that set up, download the releases listed for your
  version of Concourse from the \reference{downloads} section, plus the
  \link{CloudFoundry Postgres
  release}{http://bosh.io/releases/github.com/cloudfoundry/postgres-release},
  and \link{upload them to the BOSH
  director}{http://bosh.io/docs/uploading-releases.html}.

  Next you'll need a Concourse \link{BOSH deployment
  manifest}{http://bosh.io/docs/deployment.html}. An example manifest is
  below; you'll want to replace the \code{REPLACE_ME} bits with whatever
  values are appropriate.

  Note that the VM types, VM extensions, persistent disk type, and network names
  must come from your Cloud Config. Consult \reference{prepping-bosh} if you
  haven't set it up yet. You can retrieve your Cloud Config by running
  \code{bosh cloud-config}.

  \codeblock{yaml}{{
    ---
    name: concourse

    releases:
    - name: concourse
      version: latest
    - name: garden-runc
      version: latest
    - name: postgres
      version: latest

    stemcells:
    - alias: trusty
      os: ubuntu-trusty
      version: latest

    instance_groups:
    - name: web
      instances: 1
      # replace with a VM type from your BOSH Director's cloud config
      vm_type: REPLACE_ME
      vm_extensions:
      # replace with a VM extension from your BOSH Director's cloud config that will attach
      # this instance group to your ELB
      - REPLACE_ME
      stemcell: trusty
      azs: [z1]
      networks: [\{name: private\}]
      jobs:
      - name: atc
        release: concourse
        properties:
          # replace with your CI's externally reachable URL, e.g. https://ci.foo.com
          external_url: REPLACE_ME

          # replace with username/password, or configure GitHub auth
          basic_auth_username: REPLACE_ME
          basic_auth_password: REPLACE_ME

          # replace with your SSL cert and key
          tls_cert: REPLACE_ME
          tls_key: REPLACE_ME

          postgresql:
            database: &atc_db atc
            role: &atc_role
              # make up a role and password
              name: REPLACE_ME
              password: REPLACE_ME
      - name: tsa
        release: concourse
        properties: \{\}

    - name: db
      instances: 1
      # replace with a VM type from your BOSH Director's cloud config
      vm_type: REPLACE_ME
      stemcell: trusty
      # replace with a disk type from your BOSH Director's cloud config
      persistent_disk_type: REPLACE_ME
      azs: [z1]
      networks: [\{name: private\}]
      jobs:
      - name: postgres
        release: postgres
        properties:
          databases:
            port: 5432
            databases:
            - name: *atc_db
            roles:
            - *atc_role

    - name: worker
      instances: 1
      # replace with a VM type from your BOSH Director's cloud config
      vm_type: REPLACE_ME
      vm_extensions:
      # replace with a VM extension from your BOSH Director's cloud config that will attach
      # sufficient ephemeral storage to VMs in this instance group.
      - REPLACE_ME
      stemcell: trusty
      azs: [z1]
      networks: [\{name: private\}]
      jobs:
      - name: groundcrew
        release: concourse
        properties: \{\}
      - name: baggageclaim
        release: concourse
        properties: \{\}
      - name: garden
        release: garden-runc
        properties:
          garden:
            listen_network: tcp
            listen_address: 0.0.0.0:7777

    update:
      canaries: 1
      max_in_flight: 1
      serial: false
      canary_watch_time: 1000-60000
      update_watch_time: 1000-60000
  }}

  You may also want to consult the property descriptions for your version of
  Concourse at
  \link{bosh.io}{https://bosh.io/releases/github.com/concourse/concourse} to
  see what properties you can/should tweak.

  You may also want to consult \reference{authentication} for configuring
  something other than basic auth for the \reference{main-team}{\code{main}
  team}.

  Once you've got a manifest, just \link{deploy
  it}{https://bosh.io/docs/deploying.html}!
}

\section{
  \title{Reaching the web UI}

  This really depends on your infrastructure. If you're deploying to AWS you
  may want to configure the \code{web} VM type to register with an ELB,
  mapping port \code{80} to \code{8080}, \code{443} to \code{4443} (if you've
  configured TLS), and \code{2222} to \code{2222}.

  Otherwise you may want to configure \code{static_ips} for the \code{web}
  instance group and just reach the web UI directly.
}

\section{
  \title{Upgrading & maintaining Concourse}

  With BOSH, the deployment manifest is the source of truth. This is very
  similar to Concourse's own philosophy, where all pipeline configuration is
  defined in a single declarative document.

  So, to add more workers or web nodes, just change the \code{instances}
  value for the instance group and re-run \code{bosh deploy}.

  To upgrade, just upload the new releases and re-run \code{bosh deploy}.
}

\section{
  \title{Supporting external workers}{configuring-bosh-tsa}

  If you need workers that run outside of your BOSH managed deployment (e.g.
  for testing with iOS or in some special network), you'll need to make some
  tweaks to the default configuration of the \code{tsa} job.

  The \reference{architecture-tsa} is the entryway for workers to join the
  cluster. For every new worker key pair, the TSA will be told to authorize
  its public key, and the workers must also know the TSA's public key ahead
  of time, so they know who they're connecting to.

  \section{
    \title{Configuring the TSA's host key}

    First you'll need to remove the "magic" from the default deployment. By
    default, Concourse generates a key pair for the TSA, and gives the public
    key to the workers so they can trust the connection. This key
    occasionally gets cycled, which is fine so long as things are in one
    deployment, but once you have external workers you would have to update
    them all manually, which is annoying.

    To fix this, generate a passwordless key pair via \code{ssh-keygen}, and
    provide the following properties in your BOSH deployment manifest:

    \definitions{
      \definition{\code{host_key} on the \code{tsa} job}{
        the contents of the private key for the TSA server
      }
    }{
      \definition{\code{host_public_key} on the \code{tsa} job}{
        the public key of the TSA, for the workers to use to verify the
        connection
      }
    }

    For example (note that this manifest omits a bunch of stuff):

    \codeblock{yaml}{{
    instance_groups:
    - name: web
      jobs:
      - name: tsa
        release: concourse
        properties:
          host_key: |
            -----BEGIN RSA PRIVATE KEY-----
            <a bunch of stuff>
            -----END RSA PRIVATE KEY-----
          host_public_key: "ssh-rsa blahblahblah"
      - # ...
    }}

    After setting these properties, be sure to run \code{bosh deploy}.
  }

  \section{
    \title{Authorizing worker keys}

    We've left the worker keys auto-generated so far, which is fine for
    workers deployed alongside the TSA, as it'll also automatically authorize
    them.

    External workers however will need their own private keys, and so the TSA
    must be told to authorize them.

    To do so, set the following properties:

    \definitions{
      \definition{\code{authorized_keys} on the \code{tsa} job}{
        the array of public keys to authorize
      }
    }{
      \definition{\code{tsa.private_key} on the \code{groundcrew} job}{
        the private key for the worker to use when accessing the TSA
      }
    }{
      \definition{\code{tsa.host} and \code{tsa.host_public_key} on the \code{groundcrew} job}{
        if the worker is in a separate deployment, these must be configured
        to reach the TSA
      }
    }{
      \definition{\code{garden.forward_address} on the \code{groundcrew} job}{
        if the worker is in a separate deployment, this must be the locally-reachable Garden address to forward through the TSA; e.g.: 127.0.0.1:7777
      }
    }{
      \definition{\code{baggageclaim.forward_address} on the \code{groundcrew} job}{
        if the worker is in a separate deployment, this must be the locally-reachable Baggageclaim address to forward through the TSA; e.g.: 127.0.0.1:7788
      }
    }

    Once again, after setting these properties run \code{bosh deploy} to make
    the changes take place.
  }

  \section{
    \title{Making the TSA reachable}

    Typically the TSA and ATC will both be colocated in the same instance
    group. This way a single load balancer can be used with the following
    scheme:

    \list{
      expose port \code{443} to \code{8080} (ATC's HTTP port) via SSL
    }{
      expose port \code{2222} to \code{2222} (TSA's SSH port) via TCP
    }

    Be sure to update any relevant security group rules (or equivalent in
    non-AWS environments) to permit both access from the outside world to
    port \code{2222} on your load balancer, \italic{and} access from the load
    balancer to port \code{2222} on your TSA + ATC instances.

    The BOSH deployment manifest would then colocate both jobs together, like
    so:

    \codeblock{yaml}{{
    instance_groups:
    - name: web
      vm_type: web_lb
      jobs:
      - name: atc
        release: concourse
        # ...
      - name: tsa
        release: concourse
        # ...
    }}

    In AWS, the \code{web_lb} VM type would then configure
    \code{cloud_properties.elbs} to auto-register instances of \code{web}
    with an ELB. See \link{the AWS CPI
    docs}{https://bosh.io/docs/aws-cpi.html#resource-pools} for more
    information.
  }
}
