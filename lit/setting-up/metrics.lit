\title{Metrics}{metrics}

\use-plugin{concourse-docs}

\warn{
  This topic isn't crucial to understanding Concourse; if you're just getting
  started and have finished the \reference{installing} section, you may want to
  first move on to \reference{using-concourse}.
}

Metrics are essential in understanding how any large system is behaving and
performing. Concourse can emit metrics about both the system health itself and
about the builds that it is running. Operators can tap into these metrics in
order to observe the health of the system.

In the spirit of openness, the \link{metrics from our
deployment}{https://metrics.concourse.ci} are public. We consider it a bug to
emit anything sensitive or secret into our metrics pipeline.

Concourse components and virtual machines emit metrics to
\link{Riemann}{http://riemann.io/}. Riemann handles the processing of this
event stream and the forwarding to various \link{Time Series Databases
(TSDBs)}{https://en.wikipedia.org/wiki/Time_series_database}. To fully
understand how to take advantage of Concourse metrics you should familiarise
yourself with the \link{concepts in Riemann}{http://riemann.io/concepts.html}.
I'll wait here while you read through that (don't worry, it's not very long!).

Riemann events can contain both \italic{tags} and \italic{attributes}. We use
them for different things in Concourse metrics. We use custom attributes for
tagging all metrics with context specific information about the metric. Such as
the deployment, pipeline, or build that it relates to. We don't use the
standard build-in tags much as they are normally used for conditionals in
Riemann's stream processing which we don't rely on and don't provide the
key-value data storage that we need. This is our own convention and isn't
shared by everything in the Riemann community but we find that it works well.

\section{
  \title{Deploying the Metrics Infrastructure}

  We've made a few BOSH releases that you can collocate and deploy to get a
  similar setup to ours. If you will be emitting to DataDog then you will only
  need the Riemann release.

  \list{
    \link{Riemann Release}{https://github.com/xoebus/riemann-boshrelease}
  }{
    \link{InfluxDB Release}{https://github.com/concourse/influxdb-boshrelease}
  }{
    \link{Grafana Release}{https://github.com/vito/grafana-boshrelease}
  }

  The documentation for each of these lives in the release themselves.

  If you want to mirror our setup then you should set up Riemann to emit
  metrics to InfluxDB and then point Grafana at the same InfluxDB.

  If you set your Concourse to log everything at or above the \code{debug} level
  then all metrics will be logged as well as emitted. This is useful if you
  haven't yet set up a Riemann server.

}

\section{
  \title{Concourse Metrics}

  This reference section lists of all of the metrics that Concourse emits. We
  don't include the warning and critical levels as they will keep changing as
  we optimise the system. To find those, please refer to the source of truth:
  \link{the code}{https://github.com/concourse/atc/blob/master/metric/metrics.go}.

  \define-metric{scheduling: full duration (ms)}{
    This is the time taken (in milliseconds) to schedule an entire pipeline
    including the time taken to load the version information from the database
    and calculate the latest valid versions for each job.

    \larger{Attributes}
    \definitions{
      \definition{
        \code{pipeline}
      }{
        The pipeline which was being scheduled.
      }
    }
  }

  \define-metric{scheduling: loading versions duration (ms)}{
    This is the time taken (in milliseconds) to load the version information
    from the database.

    \larger{Attributes}
    \definitions{
      \definition{
        \code{pipeline}
      }{
        The pipeline which was being scheduled.
      }
    }
  }

  \define-metric{scheduling: job duration (ms)}{
    This is the time taken (in milliseconds) to calculate the set of valid
    input versions when scheduling a job. It is emitted once for each job per
    pipeline scheduling tick.

    \larger{Attributes}
    \definitions{
      \definition{
        \code{pipeline}
      }{
        The pipeline which was being scheduled.
      }
    }{
      \definition{
        \code{job}
      }{
        The job which was being scheduled.
      }
    }
  }

  \define-metric{worker containers}{
    The number of containers that are currently running on your workers.

    \larger{Attributes}
    \definitions{
      \definition{
        \code{worker}
      }{
        The name of the worker.
      }
    }
  }

  \define-metric{worker volumes}{
    The number of volumes that are currently present on your workers.

    \larger{Attributes}
    \definitions{
      \definition{
        \code{worker}
      }{
        The name of the worker.
      }
    }
  }

  \define-metric{build started}{
    This event is emitted when a build starts. Its value is the build ID of the
    build. However, it is most useful for annotating your metrics with the
    start and end of different jobs.

    \larger{Attributes}
    \definitions{
      \definition{
        \code{pipeline}
      }{
        The pipeline which contains the build being started.
      }
    }{
      \definition{
        \code{job}
      }{
        The job which configured the build being started.
      }
    }{
      \definition{
        \code{build_name}
      }{
        The name of the build being started. (Remember that build numbers in
        Concourse are actually names and are strings).
      }
    }{
      \definition{
        \code{build_id}
      }{
        The ID of the build being started.
      }
    }
  }

  \define-metric{build finished}{
    This event is emitted when a build ends. Its value is the duration of the
    build in milliseconds. You can use this metric in conjunction with
    \code{build started} to annotate your metrics with when builds started and
    stopped.

    \larger{Attributes}
    \definitions{
      \definition{
        \code{pipeline}
      }{
        The pipeline which contains the build that finished.
      }
    }{
      \definition{
        \code{job}
      }{
        The job which configured the build that finished.
      }
    }{
      \definition{
        \code{build_name}
      }{
        The name of the build that finished. (Remember that build numbers in
        Concourse are actually names and are strings).
      }
    }{
      \definition{
        \code{build_id}
      }{
        The ID of the build that finished.
      }
    }{
      \definition{
        \code{build_status}
      }{
        The resulting status of the build; one of "succeeded", "failed",
        "errored", or "aborted".
      }
    }
  }

  \define-metric{http response time}{
    This metric is emitted for each HTTP request to an ATC (both API and web
    requests). It contains the duration (in milliseconds) for each request and
    is useful for finding slow requests.

    \larger{Attributes}
    \definitions{
      \definition{
        \code{route}
      }{
        The route which the HTTP request matched. i.e. /builds/:id
      }
    }{
      \definition{
        \code{path}
      }{
        The literal path of the HTTP request. i.e. /builds/1234
      }
    }
  }
}

\section{
  \title{Service and Infrastructure Metrics}

  You may have seen on our dashboard that we have other metrics in addition to
  those defined above. Riemann has existing tooling for gathering metrics from
  machines and other common services. These can be found in the
  \link{riemann-tools}{https://github.com/aphyr/riemann-tools} gem. We use
  \code{riemann-health}, \code{riemann-aws-rds-status}, and \code{riemann-net}.
}
