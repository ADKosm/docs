#language anatomy

\title{v0.63.0}

\list{
  We've redone how we synchronize work between ATCs. This reduces overall work
  performed (e.g. by not checking 3x as often with 3x ATCs), and should fix
  high database CPU usage, and as a result speed up pretty much everything.

  For the technical nitty-gritty, read on, otherwise skip to the next point.

  Previously we used to synchronize application-level logic via a \code{locks}
  table in Postgres: we would create a named lock row (e.g.
  \code{resource-checking:1234}), run \code{SELECT FOR UPDATE [NOWAIT]},
  and keep it until we released it by closing the transaction and removing the
  row.

  This was all well and good until we noticed that if a build runs forever, the
  database CPU usage seems to slowly crawl up to 100%, and then
  \hyperlink{https://www.youtube.com/watch?v=3m5qxZm_JqM}{the front falls off}.

  After much debugging and trying to reduce the problem smaller and smaller, it
  ultimately came down to this: if, while a lock is held, other locks are
  acquired and released, over time the CPU usage grows until the original lock
  is released. This is pretty much always going to be what happens, since
  Concourse holds a lock for the duration of a build, and when checking for new
  versions of a resource. So, for every resource, every minute, a lock would be
  acquired and released.

  The first thing we tried was to switch to
  \hyperlink{http://www.postgresql.org/docs/9.4/static/explicit-locking.html#ADVISORY-LOCKS}{Postgres Advisory Locks},
  thinking it may have something to do with the earliest transaction having to
  keep the state of the \code{locks} table in its head in the event of a
  rollback. This made the implementation much simpler, and fixed at least that
  portion of the problem (extended isolated testing showed no CPU increase with
  this approach).

  Concourse's overall DB CPU still slowly crawls up in some scenarios, but
  \italic{very} slowly (we're talking 0.8% over a month if you have a resource
  with new versions every minute). We'll tackle these remaining issues next.

  At the end of the day we ended up switching our approach altogether. Rather
  than holding a transaction open the entire time, we now heartbeat to the
  relevant object as long as work is being done to it. So, the \code{builds}
  table now has a \code{last_tracked} column, \code{pipelines} has a
  \code{last_scheduled} column, and \code{resources} has a \code{last_checked}
  column. An ATC compare-and-swaps the column to \code{now()} if
  \code{now() - last_X} is greater than the interval. This also ensures that
  across N ATCs, we only check resources once per interval.

  As a net effect, database CPU usage and number of open connections should
  both go down and be a lot more stable. Yay!
}{
  We've fixed a bug in the new scheduling algorithm that led to churning CPU
  and taking forever to determine that a job has no available inputs.
}{
  We've fixed a bug that led to extra builds being queued up if the database
  was falling over.
}{
  The \reference{fly-execute} command now accepts the flags
  (\code{--inputs-from-pipeline} and \code{--inputs-from-job}) for running a
  task with its inputs based on resources from a job in your pipeline.
}{
  The interval on which we check for new versions of a resource can now be
  configured by setting \reference{check_every} on the resource.
}{
  The \hyperlink{https://github.com/concourse/s3-resource}{S3 resource} will
  now keep on truckin' if the \code{versioned_file} it's pointed at is deleted.
  It'll start versioning again from the new latest version.
}{
  The \hyperlink{https://github.com/concourse/cf-resource}{CF resource} can now
  be given glob paths for the \code{path} and \code{manifest} parameters.
}
